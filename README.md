# üìä Benchmark de Processamento de Dados: Pandas vs. Polars vs. R

Este reposit√≥rio cont√©m os c√≥digos e resultados de um estudo comparativo de performance entre as principais bibliotecas de manipula√ß√£o de dados em Python e R. O objetivo √© analisar velocidade de leitura, consumo de mem√≥ria RAM e efici√™ncia de armazenamento (CSV vs. Parquet) em um ambiente local (notebook).

## üìÇ Estrutura do Projeto

Para reproduzir os testes, organize seus arquivos da seguinte forma:

```text
.
‚îú‚îÄ‚îÄ datasets/                   # (N√£o inclu√≠do no git) Baixe do link abaixo
‚îÇ   ‚îú‚îÄ‚îÄ giga_yellow/            # Dataset de grande volume (>1GB)
‚îÇ   ‚îú‚îÄ‚îÄ stackoverflow_wide/     # Dataset com muitas colunas
‚îÇ   ‚îî‚îÄ‚îÄ yellow_long/            # Dataset com milh√µes de linhas
‚îÇ
‚îú‚îÄ‚îÄ resultados/                 # Onde os logs brutos dos testes ser√£o salvos
‚îÇ   ‚îú‚îÄ‚îÄ A1_gigayellow_.../      # Pastas geradas automaticamente pelo script
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ codigo_transformar_dados.qmd # Script para converter os CSVs originais em Parquet
‚îú‚îÄ‚îÄ codigo_trabalho.qmd          # Script principal que executa os benchmarks
‚îú‚îÄ‚îÄ monitor_ram.py               # Script auxiliar para monitoramento de recursos em tempo real
‚îú‚îÄ‚îÄ MASTER_BENCHMARK_DATA.csv    # Tabela consolidada com todos os resultados finais
‚îî‚îÄ‚îÄ README.md
```

Dados (Download Necess√°rio)

Devido ao tamanho, os datasets brutos n√£o est√£o no GitHub. [Insira aqui o Link do seu Google Drive]

    Baixe os arquivos.

    Crie uma pasta chamada datasets na raiz do projeto.

    Extraia os dados dentro dela conforme a estrutura acima.

Como Executar

Pr√©-requisitos

    Python 3.13+ (Bibliotecas: polars, pandas, pyarrow, numpy)

    R 4.4+ (Pacotes: readr, arrow, dplyr, glue)

    Quarto (para renderizar e executar os arquivos .qmd)

Passo a Passo

    Prepara√ß√£o dos Dados: Execute o arquivo codigo_transformar_dados.qmd. Ele ir√° ler os arquivos CSV originais na pasta datasets/ e gerar as vers√µes .parquet necess√°rias para o comparativo de formatos.

    Execu√ß√£o do Benchmark: Rode o codigo_trabalho.qmd. Este script ir√°:

        Iterar sobre todos os cen√°rios (Giga, Wide, Long).

        Testar cada biblioteca (Pandas, Polars, R Base, R Readr).

        Acionar o monitor_ram.py em segundo plano para medir o pico de mem√≥ria.

        Salvar os logs individuais na pasta resultados/.

    An√°lise dos Resultados: O arquivo MASTER_BENCHMARK_DATA.csv cont√©m a compila√ß√£o final das m√©tricas (Tempo, RAM, Speedup). Voc√™ pode us√°-lo para gerar gr√°ficos ou ler as an√°lises diretamente no relat√≥rio final.

Metodologia

O estudo avaliou tr√™s cen√°rios (A1, A2, A3) focando em:

    A1 (Leitura Pura): Velocidade bruta de ingest√£o.

    A2 (Tipagem Manual): Impacto de definir o schema explicitamente.

    A3 (Sele√ß√£o de Colunas): Efici√™ncia de Projection Pushdown.

Hardware de Teste:

    Processador: Intel Core i7

    RAM: 16 GB (Limite f√≠sico intencional)

    Armazenamento: SSD NVMe 500gb

Resultados Principais (Resumo)

    Formato: O uso de Parquet gerou ganhos de at√© 25x em rela√ß√£o ao CSV.

    Performance: O Polars foi a ferramenta mais eficiente para processar CSVs gigantes, enquanto o R (readr) mostrou excelente desempenho em arquivos Parquet.

    Mem√≥ria: Em datasets >1GB, todas as bibliotecas saturaram os 16GB de RAM, evidenciando a necessidade de estrat√©gias como Lazy Evaluation ou Chunking para m√°quinas locais.

    Desenvolvido por Diego Pires, Henry Koiti Honda e Joaquim Bertoldi Nucci
