---
title: "analise_benchmark"
format: html
editor: visual
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pl
```

1 Integrantes do grupo

### 2. Motivação

Todo cientista de dados conhece a dor: você dá o "play" no código para carregar um arquivo e fica esperando minutos,às vezes horas, olhando para a tela enquanto a memória RAM do computador vai para o limite.

O problema é claro: os dados estão cada vez maiores, mas nossos notebooks continuam, na média, com os mesmos 16GB de RAM. Ferramentas clássicas que aprendemos na faculdade, como o Pandas ou o comando básico do R, foram criadas anos atrás e muitas vezes vão penar quando o arquivo é grande demais.

Hoje, fala-se muito que o **Polars** e o formato **Parquet** são a solução mágica para isso. Mas será que são mesmo?

A motivação deste estudo é simples: colocar essas ferramentas à prova na prática. Quero descobrir se vale a pena gastar tempo aprendendo uma nova biblioteca para ganhar velocidade, e se é possível trabalhar com grandes volumes de dados no meu próprio notebook, sem precisar de computadores da NASA ou pagar caro por nuvem.

### 3. Metodologia Experimental

O experimento foi estruturado para avaliar o desempenho das bibliotecas em um cenário de restrição de recursos, representativo de uma estação de trabalho local (notebook da maioria dos aspirantes à estatísticos ou cientistas de dados como nós!) , isolando variáveis de hardware e software que poderiam introduzir ruído nas medições.

**3.1. Infraestrutura:** Os experimentos foram conduzidos em um notebook MSI Cyborg 14, operando sobre Windows 11 nativo. Para evitar a variabilidade introduzida pelo escalonamento de frequência da CPU e gerenciamento de energia, o equipamento foi mantido conectado à fonte de alimentação ininterrupta (no carregador), com o perfil de energia do sistema operacional fixado em "Alta Performance". Durante as baterias de testes, processos concorrentes foram encerrados, mantendo-se ativos estritamente o ambiente de execução (R/Python) e o diretório de dados, visando minimizar a interferência no consumo de qualquer outra coisa que estivesse aberta no computador. e

As especificações de hardware incluem um processador Intel Core i7, GPU NVIDIA RTX 4050 e 16GB de memória RAM. Este último componente define o teto físico de operação para as bibliotecas *in-memory*, servindo como principal variável de estresse nos testes de volume.

**3.2. Dados e Fontes** A seleção dos datasets buscou cobrir três vetores distintos de complexidade computacional:

-   **Volume e Extensão (Giga & Long):** Utilizou-se os dados históricos de viagens de táxi de Nova Iorque (*NYC Taxi & Limousine Commission - TLC Trip Record Data*). O dataset "Giga" avalia a capacidade de vazão (*throughput*) e gerenciamento de memória próxima à saturação, enquanto o recorte "Long" foca na eficiência de processamento vetorial em milhões de linhas. São referências em benchmarks desse tipo.

-   **Dimensionalidade (Wide):** Para avaliar o *overhead* de processamento de colunas e inferência de tipos heterogêneos, utilizou-se o *Stack Overflow Annual Developer Survey*, caracterizado por sua estrutura larga e mix de dados numéricos e textuais.

**3.3. Versionamento e tecnologias:** A reprodutibilidade dos resultados fundamenta-se nas versões específicas das bibliotecas, dado que implementações recentes (como por exemplo, o backend PyArrow no Pandas 2.0) alteram significativamente a performance de leitura.

-   **Ambiente Python (3.13):** Polars 1.33.1, Pandas 2.2.2, PyArrow 19.0.0 e NumPy 2.2.2.

-   **Ambiente R (4.4.3):** readr 2.1.5, arrow 21.0.0.1, dplyr 1.1.4 e glue 1.8.0.

**3.4. Protocolo de Medição** Cada cenário experimental, definido pela Biblioteca, Formato e tipo de dataset, foi submetido a 10 execuções consecutivas. A métrica de avaliação adotada foi a mediana dos tempos de execução, estratégia utilizada para neutralizar *outliers* decorrentes de latências momentâneas do sistema operacional ou do disco. Os logs de consumo de memória foram capturados em intervalos de milissegundos para identificar picos de uso e o comportamento de *Garbage Collection*.

Foram realizadas 338 amostras de importação desses conjuntos de dados separados em A1, A2 e A3 em que A1 é a imporação comum que foi importado com todas as bibliotecas, csv e parquet. A2 importação definindo um schema prévio que foi realizadas apenas em CSV para o Dataset Yellowlong para todas as bibliotecas. E A3 que é a análise dos conjuntos de dados "removendo" algumas colunas na leitura que foi realizada da mesma forma que o A2.

```{python}
pd.set_option('display.max_columns', None)
sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['figure.figsize'] = (12, 6)
```

```{python}
df_master = pd.read_csv("MASTER_BENCHMARK_DATA.csv")
```

```{python}
# Filtro: Vamos olhar apenas para o Cenário 1 (Leitura Pura) para ser justo
# Agrupamento pelas variáveis categóricas
tabela_descritiva = df_master.groupby(['Dataset', 'Cenario', 'Formato', 'Biblioteca']).agg(
    Tempo_Medio_s=('Tempo_Segundos', 'mean'),
    Tempo_Min_s=('Tempo_Segundos', 'min'),
    Tempo_Std=('Tempo_Segundos', 'std'),  # Indica instabilidade
    RAM_Pico_GB=('RAM_Pico_GB', 'max'),   # O pior caso de RAM é o que importa para evitar OOM
    Amostras=('Iteracao', 'count')
).reset_index()

# Cálculo do Coeficiente de Variação (CV%) - para medir estabilidade percentual
# CV < 5% = Muito estável | CV > 20% = Instável
tabela_descritiva['Estabilidade_CV_Pct'] = (
    tabela_descritiva['Tempo_Std'] / tabela_descritiva['Tempo_Medio_s']
) * 100

# Ordenar para facilitar leitura
tabela_descritiva = tabela_descritiva.sort_values(by=['Dataset', 'Cenario', 'Tempo_Medio_s'])

# Exibir tabela formatada
print("--- Tabela Descritiva Geral ---")
print(tabela_descritiva)
```

# 3.1 Comparação entre performance A1

```{python}
# Correção: Usando 'Tempo_Segundos' (coluna original) e calculando a média na hora
df_41 = df_master[df_master['Cenario'] == '1_Leitura_Pura'].copy()

# Pivotagem
tabela_formatos = df_41.pivot_table(
    index=['Dataset', 'Biblioteca'], 
    columns='Formato', 
    values='Tempo_Segundos',  # <--- Nome corrigido aqui
    aggfunc='mean'            # <--- O Python calcula a média aqui
).reset_index()

# Cálculo do Speedup (Aceleração)
# Garante que as colunas existam antes de calcular (caso falte algum dado)
if 'CSV' in tabela_formatos.columns and 'Parquet' in tabela_formatos.columns:
    tabela_formatos['Speedup_X'] = tabela_formatos['CSV'] / tabela_formatos['Parquet']
else:
    print("AVISO: Faltam dados de CSV ou Parquet para calcular o Speedup.")

# Limpeza e Exibição
tabela_output = tabela_formatos.dropna(subset=['Speedup_X']).sort_values(
    by=['Dataset', 'Speedup_X'], 
    ascending=[True, False]
)

cols = ['Dataset', 'Biblioteca', 'CSV', 'Parquet', 'Speedup_X']
print("--- Tabela para Análise 3.1 ---")
# O to_string garante que o Pandas não corte as linhas no terminal
print(tabela_output[cols].round(2).to_string(index=False))
```

-   **O Salto do Pandas (16x no Giga):** O resultado mais impactante ocorre no dataset gigante com Pandas. Ao usar CSV, o Pandas é limitado pelo *Single-Threaded Parsing* (uso de apenas 1 núcleo da CPU para ler texto), resultando em 201 segundos. Ao mudar para Parquet, o gargalo de CPU desaparece, e o tempo cai para 12 segundos.

    > **Conclusão:** Para usuários de Pandas, abandonar o CSV é obrigatório em Big Data.

-   **O Paradoxo do Polars (Speedup "Menor"):** No dataset Giga, o Polars teve um ganho de "apenas" 5.4x. Isso não é um defeito, mas uma virtude: seu leitor de CSV já é extremamente otimizado (multithreaded), completando a tarefa em 51s (4x mais rápido que o Pandas em CSV). Como a base de comparação (CSV) já era baixa, o multiplicador para o Parquet (9.4s) parece menor, embora ele tenha o tempo absoluto mais rápido do teste.

-   **A Redenção do R (25x no Long):** No dataset *Yellow Long*, as bibliotecas de R (Base e Readr) mostraram a maior sensibilidade ao formato. O CSV no R sofre com alocação de memória e inferência de tipos em milhões de linhas. O uso de Parquet via biblioteca `arrow` elimina essa ineficiência, colocando o R no mesmo patamar de velocidade do Python para leitura (0.5s).

-   **Consistência em Datasets Largos:** No *StackOverflow Wide*, todas as ferramentas apresentaram ganhos consistentes entre 8x e 10x. Isso confirma que o formato colunar (Parquet) é linearmente superior para lidar com muitas colunas, evitando o custo de verificar delimitadores em milhares de campos por linha.

```{python}
# Preparação dos dados para 4.2 - Foco no Dataset Giga (Estresse)
# Agrupando por Biblioteca e Formato para pegar a Mediana do Tempo e o Máximo de RAM
df_giga_agg = df_master[
    (df_master['Dataset'] == 'Giga_Yellow') & 
    (df_master['Cenario'] == '1_Leitura_Pura')
].groupby(['Formato', 'Biblioteca']).agg(
    Tempo_Mediano_s=('Tempo_Segundos', 'median'),
    RAM_Pico_GB=('RAM_Pico_GB', 'max')
).reset_index()

# Tabela A: Ranking CSV
ranking_csv = df_giga_agg[df_giga_agg['Formato'] == 'CSV'].sort_values('Tempo_Mediano_s')
# Adiciona coluna de comparação (Quantas vezes mais lento que o 1º lugar)
melhor_tempo_csv = ranking_csv['Tempo_Mediano_s'].iloc[0]
ranking_csv['X_Vezes_Mais_Lento'] = ranking_csv['Tempo_Mediano_s'] / melhor_tempo_csv

# Tabela B: Ranking Parquet
ranking_parquet = df_giga_agg[df_giga_agg['Formato'] == 'Parquet'].sort_values('Tempo_Mediano_s')
melhor_tempo_parquet = ranking_parquet['Tempo_Mediano_s'].iloc[0]
ranking_parquet['X_Vezes_Mais_Lento'] = ranking_parquet['Tempo_Mediano_s'] / melhor_tempo_parquet

print("--- Ranking Giga CSV (Processamento Puro) ---")
print(ranking_csv[['Biblioteca', 'Tempo_Mediano_s', 'RAM_Pico_GB', 'X_Vezes_Mais_Lento']].round(2).to_string(index=False))

print("\n--- Ranking Giga Parquet (IO Puro) ---")
print(ranking_parquet[['Biblioteca', 'Tempo_Mediano_s', 'RAM_Pico_GB', 'X_Vezes_Mais_Lento']].round(2).to_string(index=False))
```

```{python}
# Preparação dos dados para 4.3 - Eficiência de Memória
# Vamos comparar o comportamento em CSV, onde a gestão de memória é mais crítica

df_memoria = df_master[
    (df_master['Cenario'] == '1_Leitura_Pura') & 
    (df_master['Formato'] == 'CSV') &
    (df_master['Dataset'].isin(['Yellow_Long', 'Giga_Yellow']))
].groupby(['Dataset', 'Biblioteca']).agg(
    RAM_Pico_GB=('RAM_Pico_GB', 'max'),
    Tempo_Mediano_s=('Tempo_Segundos', 'median')
).reset_index()

# Pivotar para comparar os Datasets lado a lado
tabela_memoria = df_memoria.pivot_table(
    index='Biblioteca', 
    columns='Dataset', 
    values='RAM_Pico_GB'
).reset_index()

# Calcular o "Overhead" (Se possível, mas vamos focar nos valores absolutos)
print("--- Pico de Uso de Memória (GB) - CSV ---")
print(tabela_memoria.round(2).to_string(index=False))
```

### 4.3. Eficiência de Memória e Limites de Hardware

A tabela do consumo de memória RAM revela o limite físico da infraestrutura de teste e a eficiência de gerenciamento de cada biblioteca. A tabela abaixo compara o pico de memória exigido para carregar os datasets em formato CSV.

#### Diagnóstico de Saturação (O Muro dos 16GB)

No dataset **Giga Yellow**, observa-se que todas as bibliotecas atingiram o teto de **15.71 GB**. Considerando que a máquina de testes possui 16GB de RAM total, este valor indica **saturação total**.

-   **Consequência:** Quando a RAM física se esgota, o Sistema Operacional inicia o processo de *Swapping* (usar o disco rígido como memória temporária). Isso explica a degradação de performance observada na seção anterior (tempos subindo de 50s para 200s).

-   **Veredito:** Para datasets desta magnitude (\>1GB compactado ou \>5GB em memória), uma máquina de 16GB é insuficiente para processamento *in-memory* tradicional (Pandas/R). O Polars, mesmo saturando, conseguiu terminar a tarefa 4x mais rápido devido à sua execução *Lazy* e otimização de threads, minimizando o impacto do Swap.

#### Comportamento em Carga Média (Yellow Long)

No dataset intermediário (**Yellow Long**), onde a memória não foi o fator limitante, observamos o comportamento "natural" das bibliotecas:

1.  **Pandas (12.8 GB):** Surpreendentemente eficiente neste cenário específico, mas próximo do limite.

2.  **Polars (14.5 GB):** Apresentou um pico de memória ligeiramente maior. Isso é comum em engines multithreaded que alocam grandes buffers iniciais para paralelizar a leitura. O Polars troca memória por velocidade (e como vimos na seção 4.1, ele foi muito mais rápido).

> **Insight Prático:** Performance de velocidade muitas vezes custa memória. O Polars "bebe" mais memória para alimentar seus núcleos de CPU simultaneamente. Em ambientes com pouca RAM (ex: containers Docker de 2GB), o Pandas (single-core) pode ser paradoxalmente mais seguro contra *crashes*, embora muito mais lento.

#### 4.4. Otimização Manual: Esforço vs. Recompensa

Além da escolha da ferramenta, nós (como bons alunos de ME315) frequentemente recorremos a otimizações manuais no código: definição explícita de esquema (*Schema Definition*) e seleção de colunas (*Column Pruning*). A análise do dataset `Yellow_Long` (CSV) revela que essas estratégias nem sempre trazem o retorno esperado.

```{python}
# Preparação dos dados para 4.4 - Otimização Manual
# Foco: Dataset Yellow_Long em CSV
df_opt = df_master[
    (df_master['Dataset'] == 'Yellow_Long') & 
    (df_master['Formato'] == 'CSV')
].copy()

# Pivotar para ter os Cenários lado a lado
tabela_opt = df_opt.pivot_table(
    index='Biblioteca', 
    columns='Cenario', 
    values='Tempo_Segundos',
    aggfunc='median'
).reset_index()

# Calcular a Redução Percentual de Tempo (Quanto % economizei?)
# Fórmula: 1 - (Tempo Otimizado / Tempo Base)
tabela_opt['Ganho_Tipagem_%'] = (1 - (tabela_opt['2_Com_Tipagem'] / tabela_opt['1_Leitura_Pura'])) * 100
tabela_opt['Ganho_Colunas_%'] = (1 - (tabela_opt['3_Apenas_Colunas'] / tabela_opt['1_Leitura_Pura'])) * 100

# Seleção e Ordenação
cols = ['Biblioteca', '1_Leitura_Pura', '2_Com_Tipagem', '3_Apenas_Colunas', 'Ganho_Tipagem_%', 'Ganho_Colunas_%']
print("--- Impacto da Otimização Manual (Tempo em Segundos e Ganho %) ---")
print(tabela_opt[cols].round(2).to_string(index=False))
```

|  |  |  |  |  |  |
|----|----|----|----|----|----|
| **Biblioteca** | **Tempo Original (s)** | **Tempo c/ Tipos (s)** | **Impacto Tipagem** | **Tempo Só Colunas (s)** | **Impacto Seleção** |
| **Polars** | 0.56 | 0.18 | ▲ 67.6% (Melhora) | 0.17 | ▲ 69.4% (Melhora) |
| **Pandas** | 3.48 | 7.19 | ▼ -106.9% (Piora) | 2.50 | ▲ 28.1% (Melhora) |
| **R Base** | 13.32 | 8.88 | ▲ 33.3% (Melhora) | 12.94 | ▲ 2.9% (Marginal) |
| **R Readr** | 12.43 | 8.49 | ▲ 31.7% (Melhora) | 12.54 | ▼ -0.9% (Piora) |

**1. O Paradoxo da Tipagem Manual** Definir os tipos de dados manualmente (`dtype={...}`) é uma prática comum para economizar memória, mas os testes mostram que ela pode **prejudicar severamente** a velocidade de leitura em certas bibliotecas.

-   **A Armadilha do Pandas:** Ao fornecer os tipos explicitamente, o tempo de leitura do Pandas **piorou 106%** (subindo de 3.48s para 7.19s). Isso ocorre porque a engine C do Pandas é altamente otimizada para inferência rápida. Ao forçar tipos, acionam-se validadores e conversores Python/Cython mais lentos, criando um gargalo de CPU desnecessário.

-   **A Necessidade do R:** Diferente do Pandas, tanto o R Base quanto o `readr` dependem da tipagem manual para performance. Ao evitar a "adivinhação" de tipos linha a linha, o R obteve um ganho de performance de **\~30%**.

-   **Polars Acelerado:** O Polars, já sendo o mais rápido (0.56s), conseguiu reduzir seu tempo para 0.18s com tipagem, mostrando que sua engine sabe aproveitar metadados para alocar memória de forma mais eficiente.

**2. Seleção de Colunas (Projection Pushdown)** A estratégia de ler apenas as colunas necessárias demonstrou ser a otimização mais segura e eficaz para ferramentas modernas.

-   **Polars (O Vencedor):** Obteve uma melhoria de **\~69%**, caindo para impressionantes **0.17s**. Isso confirma que o Polars implementa um verdadeiro *Projection Pushdown* no leitor de CSV, ignorando fisicamente o parsing de bytes das colunas não solicitadas.

-   **Pandas (Ganho Moderado):** Obteve um ganho de **28%**. Embora útil, o ganho é menor que no Polars, indicando que o Pandas ainda realiza parte do processamento da linha inteira antes de descartar os dados.

-   **R (Ineficaz em CSV):** A seleção de colunas teve impacto nulo ou negativo no R Base e `readr`. Isso sugere que, para arquivos de texto (CSV), essas bibliotecas leem a linha completa para a memória antes de filtrar as colunas, não economizando I/O ou CPU significativamente.

> **Veredito Prático:**
>
> -   Em **Pandas**: **Nunca** defina tipos manualmente visando velocidade (apenas memória). Use sempre `usecols` para filtrar colunas.
>
> -   Em **Polars**: Ambas as otimizações funcionam, mas a seleção de colunas é a "bala de prata".
>
> -   Em **R**: Se for obrigado a usar CSV, gaste tempo definindo o `colClasses` (tipos), pois é onde está o maior ganho.

```{python}
# Preparação dos dados para 4.5 - Foco em Seleção de Colunas (Projection Pushdown)
# Usando Dataset Yellow_Long (CSV)
df_cols = df_master[
    (df_master['Dataset'] == 'Yellow_Long') & 
    (df_master['Formato'] == 'CSV')
].copy()

tabela_cols = df_cols.pivot_table(
    index='Biblioteca', 
    columns='Cenario', 
    values='Tempo_Segundos',
    aggfunc='median'
).reset_index()

# Calcular a "Economia de Tempo"
tabela_cols['Economia_Tempo_s'] = tabela_cols['1_Leitura_Pura'] - tabela_cols['3_Apenas_Colunas']
tabela_cols['Eficiencia_Pushdown_%'] = (tabela_cols['Economia_Tempo_s'] / tabela_cols['1_Leitura_Pura']) * 100

# Formatação
tabela_final_a3 = tabela_cols[[
    'Biblioteca', 
    '1_Leitura_Pura', 
    '3_Apenas_Colunas', 
    'Eficiencia_Pushdown_%'
]].sort_values('Eficiencia_Pushdown_%', ascending=False)

# Ajuste visual
tabela_final_a3.columns = ['Biblioteca', 'Leitura Total (s)', 'Leitura Parcial (s)', 'Ganho de Performance (%)']
tabela_final_a3['Ganho de Performance (%)'] = tabela_final_a3['Ganho de Performance (%)'].apply(lambda x: f"▲ {x:.1f}%" if x > 0 else f"▼ {x:.1f}%")
tabela_final_a3['Leitura Total (s)'] = tabela_final_a3['Leitura Total (s)'].round(2)
tabela_final_a3['Leitura Parcial (s)'] = tabela_final_a3['Leitura Parcial (s)'].round(2)

print("--- Tabela 4.5: Eficiência de Projection Pushdown ---")
print(tabela_final_a3.to_markdown(index=False))
```

#### 4.5. Inteligência de Leitura: O Efeito *Projection Pushdown*

Uma das otimizações mais críticas em Big Data é o *Column Pruning* (podar colunas): instruir a ferramenta a ler apenas as colunas necessárias para a análise, ignorando o resto.

A tabela abaixo mede a eficiência de cada biblioteca em aplicar essa técnica no dataset CSV `Yellow_Long`. O indicador "Ganho de Performance" revela se a biblioteca realmente ignorou os dados desnecessários (*Projection Pushdown*) ou se leu tudo para filtrar depois.

|             |                   |                     |                          |
|-------------|-------------------|---------------------|--------------------------|
| Biblioteca  | Leitura Total (s) | Leitura Parcial (s) | Ganho de Performance (%) |
| **Polars**  | 0.56              | 0.17                | ▲ 69.4%                  |
| **Pandas**  | 3.48              | 2.50                | ▲ 28.1%                  |
| **R Base**  | 13.32             | 12.94               | ▲ 2.9%                   |
| **R Readr** | 12.43             | 12.54               | ▼ -0.9%                  |

**Análise Técnica**

1.  **Polars e o *True Pushdown*:** O Polars apresentou uma redução de tempo drástica (**\~70%**). Isso comprova que sua engine de leitura é capaz de realizar *Projection Pushdown* físico em arquivos CSV. Ao identificar que apenas certas colunas são necessárias, o Polars pula o *parsing* (interpretação de texto para binário) das colunas ignoradas, economizando ciclos de CPU massivos. É uma leitura "inteligente".

2.  **Pandas e o *Post-Read Filter*:** O Pandas obteve um ganho moderado (**28%**). Embora útil, o ganho não é proporcional à redução de dados. Isso indica que o Pandas ainda precisa escanear a estrutura da linha e realizar alocações parciais antes de descartar as colunas indesejadas. O ganho vem mais da economia de memória RAM do que de processamento bruto.

3.  **R (Base e Readr): Leitura Cega:** As ferramentas de R apresentaram ganho nulo ou marginal. Isso sugere que, para o formato CSV, essas bibliotecas realizam a leitura completa do arquivo para a memória (ou buffer) antes de selecionar as colunas. Ou seja, pedir 2 colunas ou 20 colunas leva o mesmo tempo de processamento; a economia ocorre apenas na memória final ocupada, não na velocidade de ingestão.

> **Conclusão:** Para pipelines de alta performance onde apenas um subconjunto dos dados é necessário, o **Polars** é a única ferramenta que transforma essa redução lógica em ganho de velocidade real. Em R e Pandas, selecionar colunas ajuda na memória, mas não resolve gargalos de tempo de leitura em CSV.

### 5. Discussão: Developer Experience (DX)

Enquanto as seções anteriores focaram na eficiência computacional, esta discussão aborda a "Experiência do Desenvolvedor" (DX). A escolha de uma ferramenta não ocorre por nada: ela envolve um *trade-off* entre a velocidade de execução do código e a velocidade de escrita do mesmo.

**5.1. A Curva de Adoção do Polars** A transição do Pandas para o Polars não se comporta como uma simples troca de sintaxe, mas assemelha-se à de fato estar aprendendo uma nova linguagem. O Polars embora muito bem estruturado e robusto,impõe uma barreira de entrada significativa para quem possui memória muscular desenvolvida no estilo imperativo do Pandas (Caso de quem escreveu isso aqui).

Durante os experimentos, observou-se que o rigor do Polars quanto à tipagem (definir o schema (tiposde colunas)) de dados atua como uma faca de dois gumes que pode ser um pouco chato no começo. Em datasets volumosos (Giga Yellow), a inferência de tipos estrita gerou exceções que impediram a leitura imediata, exigindo intervenções manuais de configuração antes mesmo do processamento iniciar. Em contrapartida, a permissividade do Pandas permitiu uma ingestão inicial mais fluida ("plug-and-play"), ainda que ao custo de performance posterior. Na nossa concepção do grupo, o polars de fato é uma linguagem muito mais robusta quando se está trabalhando em um ecossitema que exige um maior nível de profissionalidade e eficiência do código importa mais do que a facilidade de escrita e de utilização. Porém, (Fala do Diego) eu ainda continuo importanto o pandas toda vez que abro um arquivo em python novo, é muito mais prático para tarefas que não justificam o ganho de eficiência mostrada anteriormente.

**5.2. A Coesão do Ecossistema R** Diferente da fragmentação que por vezes ocorre ao integrar bibliotecas de alta performance em outros ambientes, a utilização do `readr` e `arrow` no ecossistema R demonstrou consistência. A integração dessas ferramentas (backend C++, isso justifica a alta velocidade)com a sintaxe do R manteve-se transparente, não exigindo alterações drásticas no fluxo de trabalho habitual de um estatístico.

**5.3. Considerações sobre o Aprendizado** A análise qualitativa sugere que a dificuldade associada ao Polars. A arquitetura da biblioteca força boas práticas de engenharia de dados desde a leitura. Conclui-se que, se o Polars fosse a primeira ferramenta apresentada a um estudante, a percepção de dificuldade seria mitigada, estabelecendo um padrão mental mais eficiente desde o início. O esforço de migração gera uma recompensa assimétrica: o custo cognitivo inicial é alto, mas o ganho de escalabilidade e a redução de dívida técnica no longo prazo são substanciais. No caso de R, não há nenhuma justificativa em utilizar a Base do R em contrapartida ao ReadR, ambos tem uma usabilidade muito parecida e o ganho de performance é muito grande entre ambas. Entre a seleção de R e Python, é uma coisa muito pessoal e depende muito da área de atuação do Cientista de Dados/Estatístico, porém, se a necessidade é o "Importador de bibliotecas mais rápido" sempre opte pelo Polars.\

**5.4. O Declínio do CSV** A persistência do CSV como padrão na indústria revela um conflito entre legibilidade humana e eficiência de máquina. Embora o CSV ofereça a vantagem da facilidade de uso, podendo ser aberto desde o Bloco de Notas até o Excel, ele transfere a vantagem da estruturação simples para o momento da leitura (que precisa decodificar todo o arquivo).

Sob a ótica da Experiência do Desenvolvedor (DX), o formato Parquet se mostrou não apenas superior em performance, mas também em robustez. Grande parte da fricção relatada com o Polars (a necessidade de configurar tipos manuais e tratar erros de inferência) desaparece ao utilizar Parquet. Diferente do CSV, que é "esquecido" de sua estrutura a cada salvamento, o Parquet preserva os metadados e o esquema (*schema*) do dataset. Isso significa que um arquivo salvo como `Int64` será lido como `Int64`, eliminando a adivinhação custosa e propensa a erros que as bibliotecas precisam realizar a cada leitura de texto.

A análise sugere uma mudança de paradigma no fluxo de trabalho: o CSV deve ser relegado estritamente às pontas do pipeline (recebimento de dados brutos de terceiros ou entrega final para relatórios ou para o cliente). Para todas as etapas intermediárias de armazenamento e processamento, o Parquet elimina ambiguidades técnicas, agindo como um contrato de dados estável entre diferentes sessões e ferramentas.

Utilize Parquet!

### 6. Guia Prático de Seleção (o motivador de todo trabalho)

Baseado nos números e na dor de cabeça que tivemos para rodar tudo isso, aqui estão as 6 regras para você não perder tempo em seus projetos:

**1. A Regra do Novo Padrão**

> **"Vai começar agora? Comece com Polars."** Se o seu dado cabe na memória RAM, o Polars deve ser sua primeira escolha. Aprender a sintaxe dele é chato no começo? É. Mas ele é 4x mais rápido que o Pandas e te protege de erros bobos de tipagem. O investimento se paga rápido.

**2. A Regra do Legado**

> **"Não conserte o que não está lento."** Se você tem um script em Pandas que roda em 10 segundos, não perca 2 horas reescrevendo ele em Polars para ganhar 5 segundos. O Pandas ainda é ótimo para protótipos rápidos e datasets pequenos (abaixo de 500MB). Use o tempo para analisar dados, não para refatorar código.

**3. A Regra do Formato!**

> **"CSV é para transporte, Parquet é para trabalho."** A maior otimização que você pode fazer não é trocar de linguagem, é trocar de arquivo. Nossos testes mostraram ganhos de até **25x** só de sair do CSV para o Parquet. Use CSV só se o cliente exigir; internamente, salve tudo em Parquet.

**4. A Regra da Otimização**

> **"Selecione colunas sempre. Tipagem manual? Depende."** Se for obrigado a ler um CSV gigante:

-   **Sempre** diga para a biblioteca quais colunas você quer (`select` ou `usecols`). Isso economiza muita memória.

-   **Cuidado no Pandas:** Definir os tipos na mão (`int`, `float`) fez o Pandas ficar **mais lento** nos nossos testes. Deixe ele se virar sozinho.

-   **No Polars e R:** Definir tipos na mão ajuda e deixa tudo mais rápido.

**5. A Regra do Usuário de R**

> **"Aposente o `read.csv`."** Se você usa R, esqueça que a função nativa `read.csv` existe. Ela é lenta demais. Use sempre os pacotes `readr` ou `arrow`. Com eles, o R fica tão competitivo quanto o Python.

**6. A Regra da Guerra (Python vs. R)**

> **"O R não morreu, ele só precisa das ferramentas certas."** Muita gente diz que Python é muito mais rápido que R. Nossos testes provaram que isso é **meia verdade**.

-   **No CSV (Força Bruta):** O Python (com Polars) ganha de lavada. É a melhor ferramente para de fato "mastigar" texto.

-   **No Parquet (Leitura Otimizada):** O R (com `readr`) empatou e até ganhou em alguns casos.

-   **Resumo:** Se o dado já está limpo e em Parquet, a linguagem não importa tanto. Use a que você gosta mais e que fará mais sentido com o resto do seu trabalho.

### 7. Conclusão: Eficiência como Sobrevivência no Limite do Hardware

Este estudo iniciou-se com uma pergunta sobre velocidade ("Quem é mais rápido?"), mas os dados revelaram uma questão mais crítica sobre viabilidade ("Quem consegue terminar a tarefa?").

Ao submeter um notebook de alta performance (i7, 16GB RAM) a um dataset de escala Gigabyte, observamos o fenômeno do **"Muro do Hardware"**. Todas as bibliotecas testadas atingiram o teto físico de 15.71 GB de memória RAM no teste de estresse. Nesse cenário limite, a eficiência da ferramenta deixa de ser uma questão de economizar segundos e passa a determinar se o workflow é executável ou se resultará em um erro de *Out of Memory* (OOM).

**A Sobrevivência Local** Para o Cientista de Dados que opera localmente, fora de clusters infinitos na nuvem , a conclusão é clara:

1.  **Otimização é Obrigatória:** O uso de formatos colunares (**Parquet**) e técnicas de leitura seletiva (**Projection Pushdown**) não são apenas "boas práticas"; são os únicos mecanismos que permitem processar volumes de dados superiores à memória física disponível, evitando o gargalo de *Swap* em disco.

2.  **A Hegemonia da Arquitetura:** O Polars provou ser a ferramenta mais apta para este cenário de recursos escassos. Sua capacidade de gerenciar memória agressivamente e processar dados em paralelo permitiu que ele entregasse performance mesmo quando a máquina estava saturada, algo que as engines *single-core* (Pandas e R Base) lutaram para acompanhar.

**O Horizonte de Escalabilidade** Embora as ferramentas modernas como Polars e `readr` estendam significativamente a vida útil do hardware local, permitindo análises de Big Data em um laptop, existe um limite físico intransponível.

Quando o volume de dados ultrapassa consistentemente a barreira da RAM (mesmo com otimizações) ou quando o tempo de processamento inviabiliza a iteração ágil, a solução deixa de ser "trocar a biblioteca" e passa a ser "trocar a infraestrutura". Neste ponto, os conceitos aprendidos aqui (Parquet, execução Lazy, Tipagem Estrita) tornam-se a base necessária para migrar para frameworks de processamento distribuído, como **Apache Spark** ou **Dask**, onde a lógica de eficiência permanece a mesma, mas a escala torna-se horizontal.
